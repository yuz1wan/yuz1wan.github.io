---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I‚Äôm a junior student studying at Shanghai Jiao Tong University, major in Micro Electronics.  
I'm an intern student at the [RHOS Group](https://mvig-rhos.com/) at [MVIG Lab](https://www.mvig.org/), Shanghai Jiao Tong University, under the supervision of Prof. [Yong-Lu Li](https://dirtyharrylyl.github.io/) and Prof. [Cewu Lu](https://www.mvig.org/).

My research interests mainly lie in Computer Vision and Robotics.

# üî• News
- *2024.02*: &nbsp;üéâüéâ Our work [Video Distillation](https://github.com/yuz1wan/video_distillation) will appear at **CVPR 2024**.

# üìù Publications 
equal contribution: *  
corresponding author: #
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/video_distill.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement](https://arxiv.org/abs/2312.00362)

**Ziyu Wang**\*, Yue Xu\*, Cewu Lu, Yong-Lu Li#

[**Project**](https://github.com/yuz1wan/video_distillation)  ![Stars](https://img.shields.io/github/stars/yuz1wan/video_distillation?color=yellow&label=Stars)<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- In this work, we provide the first systematic study of video distillation and introduce a taxonomy to categorize temporal compression.
- We introduce a novel taxonomy for temporal condensation in video distillation methods, which guides our and future works.
- We propose a novel paradigm, enabling existing image distillation techniques to achieve improved results when applied to video distillation while using an even smaller memory storage budget. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/distill_pruning.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection](https://arxiv.org/abs/2305.18381)

Yue Xu, Yong-Lu Li#, Kaitong Cui, **Ziyu Wang**, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang

[**Project**](https://github.com/silicx/GoldFromOres)  ![Stars](https://img.shields.io/github/stars/silicx/GoldFromOres?color=yellow&label=Stars)<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- This is the very first work that systematically study the data redundancy in the dataset distillation. 
- We propose multiple effective criteria for pruning, and we hope our observation, analysis and empirical results could provide deeper insight into the internal mechanism of dataset distillation and neural network training.
</div>
</div>

# üéñ Honors and Awards
- *2023.10* **China Optics Valley Scholarship - Optoelectronic Information Award** (No more than 30 undergraduate students in SJTU).
- *2023.10* Zhiyuan Honors Scholarship (top 5%).
- *2022.10* Zhiyuan Honors Scholarship (top 5%).

# üìñ Educations
- *2021.09 - 2025.06 (expected)*, B.S. major in Micro Electronics, Shanghai Jiao Tong University
  - **Zhiyuan Honor Program of Engineering** (an elite program for Top 5% talented students)

# üíª Internships
- *2023.02 - Present*, [MVIG Lab](https://www.mvig.org/), [RHOS Group](https://mvig-rhos.com/), China.